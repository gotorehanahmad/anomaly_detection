{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# ignore potential warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rseed = 1234 \n",
    "rd.seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "torch.manual_seed(rseed) \n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    torch.cuda.manual_seed(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_dataset = pd.read_csv('./data/fraud_dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BELNR</th>\n",
       "      <th>WAERS</th>\n",
       "      <th>BUKRS</th>\n",
       "      <th>KTOSL</th>\n",
       "      <th>PRCTR</th>\n",
       "      <th>BSCHL</th>\n",
       "      <th>HKONT</th>\n",
       "      <th>DMBTR</th>\n",
       "      <th>WRBTR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288203</td>\n",
       "      <td>C3</td>\n",
       "      <td>C31</td>\n",
       "      <td>C9</td>\n",
       "      <td>C92</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>280979.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324441</td>\n",
       "      <td>C1</td>\n",
       "      <td>C18</td>\n",
       "      <td>C7</td>\n",
       "      <td>C76</td>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>129856.53</td>\n",
       "      <td>243343.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133537</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>C2</td>\n",
       "      <td>C20</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>957463.97</td>\n",
       "      <td>3183838.41</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331521</td>\n",
       "      <td>C4</td>\n",
       "      <td>C48</td>\n",
       "      <td>C9</td>\n",
       "      <td>C95</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>2681709.51</td>\n",
       "      <td>28778.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375333</td>\n",
       "      <td>C5</td>\n",
       "      <td>C58</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>910514.49</td>\n",
       "      <td>346.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BELNR WAERS BUKRS KTOSL PRCTR BSCHL HKONT       DMBTR       WRBTR    label\n",
       "0  288203    C3   C31    C9   C92    A3    B1   280979.60        0.00  regular\n",
       "1  324441    C1   C18    C7   C76    A1    B2   129856.53   243343.00  regular\n",
       "2  133537    C1   C19    C2   C20    A1    B3   957463.97  3183838.41  regular\n",
       "3  331521    C4   C48    C9   C95    A2    B1  2681709.51    28778.00  regular\n",
       "4  375333    C5   C58    C1   C19    A3    B1   910514.49      346.00  regular"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533009, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regular    532909\n",
       "global         70\n",
       "local          30\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ad_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attr = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'WAERS', 'BUKRS']\n",
    "ad_dataset_categ_transformed = pd.get_dummies(ad_dataset[categorical_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ad_dataset[numeric_attr] + 1e-7\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "ad_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_subset_transformed = pd.concat([ad_dataset_categ_transformed, ad_dataset_numeric_attr], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533009, 618)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 512, out 256\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 256, out 128\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 128, out 64\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 64, out 32\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 32, out 16\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 7 - in 16, out 8\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 8, out 4\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 4, out 3\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x)) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    encoder_train = encoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191001-09:34:32] encoder architecture:\n",
      "\n",
      "encoder(\n",
      "  (encoder_L1): Linear(in_features=618, out_features=512, bias=True)\n",
      "  (encoder_R1): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (encoder_R2): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (encoder_R3): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (encoder_R4): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L5): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (encoder_R5): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L6): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (encoder_R6): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L7): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (encoder_R7): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L8): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (encoder_R8): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (encoder_L9): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (encoder_R9): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 4\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 4, out 8\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 8, out 16\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 32\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 32, out 64\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 64, out 128\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 7 - in 128, out 256\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 256, out 512\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 512, out 618\n",
    "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191001-09:34:32] decoder architecture:\n",
      "\n",
      "decoder(\n",
      "  (decoder_L1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (decoder_R1): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L2): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (decoder_R2): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L3): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (decoder_R3): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L4): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (decoder_R4): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L5): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (decoder_R5): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L6): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (decoder_R6): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L7): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (decoder_R7): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L8): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (decoder_R8): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (decoder_L9): Linear(in_features=512, out_features=618, bias=True)\n",
      "  (decoder_R9): LeakyReLU(negative_slope=0.4, inplace)\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init training network classes / architectures\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    decoder_train = decoder().cuda()\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 1e-3\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 8\n",
    "mini_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(ad_subset_transformed.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# note: we set num_workers to zero to retrieve deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191001-09:34:55] training status, epoch: [0001/0008], batch: 1000, loss: 0.0192, mode: GPU, time required: 0:00:07.556938\n",
      "[LOG 20191001-09:35:03] training status, epoch: [0001/0008], batch: 2000, loss: 0.0158, mode: GPU, time required: 0:00:07.201424\n",
      "[LOG 20191001-09:35:10] training status, epoch: [0001/0008], batch: 3000, loss: 0.0117, mode: GPU, time required: 0:00:07.325514\n",
      "[LOG 20191001-09:35:17] training status, epoch: [0001/0008], batch: 4000, loss: 0.0098, mode: GPU, time required: 0:00:07.452976\n",
      "[LOG 20191001-09:35:29] training status, epoch: [0001/0008], loss: 0.0099873040\n",
      "[LOG 20191001-09:35:36] training status, epoch: [0002/0008], batch: 1000, loss: 0.0082, mode: GPU, time required: 0:00:07.358299\n",
      "[LOG 20191001-09:35:44] training status, epoch: [0002/0008], batch: 2000, loss: 0.0076, mode: GPU, time required: 0:00:07.626276\n",
      "[LOG 20191001-09:35:51] training status, epoch: [0002/0008], batch: 3000, loss: 0.0066, mode: GPU, time required: 0:00:07.005185\n",
      "[LOG 20191001-09:35:58] training status, epoch: [0002/0008], batch: 4000, loss: 0.0077, mode: GPU, time required: 0:00:07.375204\n",
      "[LOG 20191001-09:36:11] training status, epoch: [0002/0008], loss: 0.0062850188\n",
      "[LOG 20191001-09:36:17] training status, epoch: [0003/0008], batch: 1000, loss: 0.006, mode: GPU, time required: 0:00:06.775188\n",
      "[LOG 20191001-09:36:24] training status, epoch: [0003/0008], batch: 2000, loss: 0.0079, mode: GPU, time required: 0:00:06.909862\n",
      "[LOG 20191001-09:36:31] training status, epoch: [0003/0008], batch: 3000, loss: 0.0057, mode: GPU, time required: 0:00:06.307081\n",
      "[LOG 20191001-09:36:37] training status, epoch: [0003/0008], batch: 4000, loss: 0.0057, mode: GPU, time required: 0:00:06.323947\n",
      "[LOG 20191001-09:36:49] training status, epoch: [0003/0008], loss: 0.0076645338\n",
      "[LOG 20191001-09:36:57] training status, epoch: [0004/0008], batch: 1000, loss: 0.0049, mode: GPU, time required: 0:00:07.440244\n",
      "[LOG 20191001-09:37:04] training status, epoch: [0004/0008], batch: 2000, loss: 0.0047, mode: GPU, time required: 0:00:07.090255\n",
      "[LOG 20191001-09:37:11] training status, epoch: [0004/0008], batch: 3000, loss: 0.0041, mode: GPU, time required: 0:00:06.941356\n",
      "[LOG 20191001-09:37:17] training status, epoch: [0004/0008], batch: 4000, loss: 0.0043, mode: GPU, time required: 0:00:06.874243\n",
      "[LOG 20191001-09:37:30] training status, epoch: [0004/0008], loss: 0.0047457498\n",
      "[LOG 20191001-09:37:37] training status, epoch: [0005/0008], batch: 1000, loss: 0.0039, mode: GPU, time required: 0:00:07.191664\n",
      "[LOG 20191001-09:37:44] training status, epoch: [0005/0008], batch: 2000, loss: 0.0038, mode: GPU, time required: 0:00:07.151675\n",
      "[LOG 20191001-09:37:51] training status, epoch: [0005/0008], batch: 3000, loss: 0.0052, mode: GPU, time required: 0:00:07.067208\n",
      "[LOG 20191001-09:37:58] training status, epoch: [0005/0008], batch: 4000, loss: 0.0037, mode: GPU, time required: 0:00:07.103229\n",
      "[LOG 20191001-09:38:10] training status, epoch: [0005/0008], loss: 0.0045759645\n",
      "[LOG 20191001-09:38:18] training status, epoch: [0006/0008], batch: 1000, loss: 0.0042, mode: GPU, time required: 0:00:07.377698\n",
      "[LOG 20191001-09:38:25] training status, epoch: [0006/0008], batch: 2000, loss: 0.0037, mode: GPU, time required: 0:00:07.239339\n",
      "[LOG 20191001-09:38:32] training status, epoch: [0006/0008], batch: 3000, loss: 0.0041, mode: GPU, time required: 0:00:07.119859\n",
      "[LOG 20191001-09:38:39] training status, epoch: [0006/0008], batch: 4000, loss: 0.0032, mode: GPU, time required: 0:00:07.198198\n",
      "[LOG 20191001-09:38:51] training status, epoch: [0006/0008], loss: 0.0037260407\n",
      "[LOG 20191001-09:38:59] training status, epoch: [0007/0008], batch: 1000, loss: 0.0035, mode: GPU, time required: 0:00:07.663209\n",
      "[LOG 20191001-09:39:06] training status, epoch: [0007/0008], batch: 2000, loss: 0.004, mode: GPU, time required: 0:00:07.280948\n",
      "[LOG 20191001-09:39:13] training status, epoch: [0007/0008], batch: 3000, loss: 0.0045, mode: GPU, time required: 0:00:07.435035\n",
      "[LOG 20191001-09:39:21] training status, epoch: [0007/0008], batch: 4000, loss: 0.0036, mode: GPU, time required: 0:00:07.165838\n",
      "[LOG 20191001-09:39:32] training status, epoch: [0007/0008], loss: 0.0035007193\n",
      "[LOG 20191001-09:39:40] training status, epoch: [0008/0008], batch: 1000, loss: 0.0045, mode: GPU, time required: 0:00:07.100983\n",
      "[LOG 20191001-09:39:46] training status, epoch: [0008/0008], batch: 2000, loss: 0.0033, mode: GPU, time required: 0:00:06.616858\n",
      "[LOG 20191001-09:39:53] training status, epoch: [0008/0008], batch: 3000, loss: 0.0038, mode: GPU, time required: 0:00:07.016406\n",
      "[LOG 20191001-09:40:00] training status, epoch: [0008/0008], batch: 4000, loss: 0.0032, mode: GPU, time required: 0:00:07.326390\n",
      "[LOG 20191001-09:40:13] training status, epoch: [0008/0008], loss: 0.0034663924\n"
     ]
    }
   ],
   "source": [
    "# init collection of mini-batch losses\n",
    "losses = []\n",
    "\n",
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "    \n",
    "    # determine if CUDA is available at compute node\n",
    "    if(torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "        \n",
    "        # set networks / models in GPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "\n",
    "    # start timer\n",
    "    start_time = datetime.now()\n",
    "        \n",
    "    # iterate over all mini-batches\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== (1) forward pass ===================================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "        \n",
    "        # =================== (2) compute reconstruction loss ====================\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
    "        \n",
    "        # =================== (3) backward pass ==================================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # =================== (4) update model parameters ========================\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== monitor training progress ==========================\n",
    "\n",
    "        # print training progress each 1'000 mini-batches\n",
    "        if mini_batch_count % 1000 == 0:\n",
    "            \n",
    "            # print the training mode: either on GPU or CPU\n",
    "            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'\n",
    "            \n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            end_time = datetime.now() - start_time\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))\n",
    "\n",
    "            # reset timer\n",
    "            start_time = datetime.now()\n",
    "\n",
    "    # =================== evaluate model performance =============================\n",
    "    \n",
    "    # set networks in evaluation mode (don't apply dropout)\n",
    "    encoder_train.cpu().eval()\n",
    "    decoder_train.cpu().eval()\n",
    "\n",
    "    # reconstruct encoded transactional data\n",
    "    reconstruction = decoder_train(encoder_train(data))\n",
    "    \n",
    "    # determine reconstruction loss - all transactions\n",
    "    reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "            \n",
    "    # collect reconstruction loss\n",
    "    losses.extend([reconstruction_loss_all.item()])\n",
    "    \n",
    "    # print reconstuction loss results\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss_all.item()))\n",
    "\n",
    "    # =================== save model snapshot to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AENN training performance')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training progress\n",
    "plt.plot(range(0, len(losses)), losses)\n",
    "plt.xlabel('[training epoch]')\n",
    "plt.xlim([0, len(losses)])\n",
    "plt.ylabel('[reconstruction-error]')\n",
    "#plt.ylim([0.0, 1.0])\n",
    "plt.title('AENN training performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = \"ep_8_encoder_model.pth\"\n",
    "decoder_model_name = \"ep_8_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191001-09:40:24] collected reconstruction loss of: 533009/533009 transactions\n",
      "[LOG 20191001-09:40:24] reconstruction loss: 0.0034663924\n"
     ]
    }
   ],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20191001-09:40:24] collected individual reconstruction loss of: 000000/533009 transactions\n",
      "[LOG 20191001-09:40:29] collected individual reconstruction loss of: 100000/533009 transactions\n",
      "[LOG 20191001-09:40:34] collected individual reconstruction loss of: 200000/533009 transactions\n",
      "[LOG 20191001-09:40:39] collected individual reconstruction loss of: 300000/533009 transactions\n",
      "[LOG 20191001-09:40:44] collected individual reconstruction loss of: 400000/533009 transactions\n",
      "[LOG 20191001-09:40:49] collected individual reconstruction loss of: 500000/533009 transactions\n"
     ]
    }
   ],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5e3c32a080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# assign unique id to transactions\n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = plot_data[label == 'regular']\n",
    "global_outliers = plot_data[label == 'global']\n",
    "local_outliers = plot_data[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", label='regular') # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", label='global') # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"^\", label='local') # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_dataset['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BELNR</th>\n",
       "      <th>WAERS</th>\n",
       "      <th>BUKRS</th>\n",
       "      <th>KTOSL</th>\n",
       "      <th>PRCTR</th>\n",
       "      <th>BSCHL</th>\n",
       "      <th>HKONT</th>\n",
       "      <th>DMBTR</th>\n",
       "      <th>WRBTR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35691</th>\n",
       "      <td>532967</td>\n",
       "      <td>M07</td>\n",
       "      <td>S42</td>\n",
       "      <td>J81</td>\n",
       "      <td>A14</td>\n",
       "      <td>O49</td>\n",
       "      <td>Z01</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35944</th>\n",
       "      <td>532917</td>\n",
       "      <td>J82</td>\n",
       "      <td>Q22</td>\n",
       "      <td>P32</td>\n",
       "      <td>Z74</td>\n",
       "      <td>Y19</td>\n",
       "      <td>K27</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41925</th>\n",
       "      <td>532923</td>\n",
       "      <td>T17</td>\n",
       "      <td>R47</td>\n",
       "      <td>D84</td>\n",
       "      <td>F38</td>\n",
       "      <td>I81</td>\n",
       "      <td>D68</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52048</th>\n",
       "      <td>532958</td>\n",
       "      <td>Q82</td>\n",
       "      <td>S54</td>\n",
       "      <td>J53</td>\n",
       "      <td>T46</td>\n",
       "      <td>Y42</td>\n",
       "      <td>S06</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71036</th>\n",
       "      <td>532939</td>\n",
       "      <td>B00</td>\n",
       "      <td>O64</td>\n",
       "      <td>T41</td>\n",
       "      <td>Y68</td>\n",
       "      <td>H15</td>\n",
       "      <td>L79</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88224</th>\n",
       "      <td>532934</td>\n",
       "      <td>D88</td>\n",
       "      <td>B37</td>\n",
       "      <td>D51</td>\n",
       "      <td>F69</td>\n",
       "      <td>N50</td>\n",
       "      <td>M13</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90831</th>\n",
       "      <td>532955</td>\n",
       "      <td>P03</td>\n",
       "      <td>M90</td>\n",
       "      <td>U27</td>\n",
       "      <td>R22</td>\n",
       "      <td>S15</td>\n",
       "      <td>N95</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91688</th>\n",
       "      <td>532932</td>\n",
       "      <td>R07</td>\n",
       "      <td>U18</td>\n",
       "      <td>V96</td>\n",
       "      <td>L28</td>\n",
       "      <td>K79</td>\n",
       "      <td>K77</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93972</th>\n",
       "      <td>532964</td>\n",
       "      <td>L82</td>\n",
       "      <td>G45</td>\n",
       "      <td>G19</td>\n",
       "      <td>W37</td>\n",
       "      <td>A49</td>\n",
       "      <td>U45</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102552</th>\n",
       "      <td>532937</td>\n",
       "      <td>P37</td>\n",
       "      <td>S83</td>\n",
       "      <td>S23</td>\n",
       "      <td>U16</td>\n",
       "      <td>N35</td>\n",
       "      <td>C42</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958502e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113179</th>\n",
       "      <td>532969</td>\n",
       "      <td>H01</td>\n",
       "      <td>E65</td>\n",
       "      <td>O50</td>\n",
       "      <td>F20</td>\n",
       "      <td>M83</td>\n",
       "      <td>V78</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119338</th>\n",
       "      <td>532930</td>\n",
       "      <td>G92</td>\n",
       "      <td>W69</td>\n",
       "      <td>K78</td>\n",
       "      <td>J13</td>\n",
       "      <td>W62</td>\n",
       "      <td>S49</td>\n",
       "      <td>9.244554e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131910</th>\n",
       "      <td>532922</td>\n",
       "      <td>O43</td>\n",
       "      <td>R07</td>\n",
       "      <td>N80</td>\n",
       "      <td>J39</td>\n",
       "      <td>T90</td>\n",
       "      <td>V92</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172868</th>\n",
       "      <td>532973</td>\n",
       "      <td>W25</td>\n",
       "      <td>G35</td>\n",
       "      <td>K00</td>\n",
       "      <td>C11</td>\n",
       "      <td>B38</td>\n",
       "      <td>C25</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179462</th>\n",
       "      <td>532972</td>\n",
       "      <td>S31</td>\n",
       "      <td>G87</td>\n",
       "      <td>A19</td>\n",
       "      <td>H01</td>\n",
       "      <td>U81</td>\n",
       "      <td>Z02</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198583</th>\n",
       "      <td>532927</td>\n",
       "      <td>K11</td>\n",
       "      <td>Q00</td>\n",
       "      <td>R19</td>\n",
       "      <td>U99</td>\n",
       "      <td>F24</td>\n",
       "      <td>N10</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958506e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199041</th>\n",
       "      <td>532968</td>\n",
       "      <td>K99</td>\n",
       "      <td>T17</td>\n",
       "      <td>C84</td>\n",
       "      <td>L74</td>\n",
       "      <td>T28</td>\n",
       "      <td>N47</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958501e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213105</th>\n",
       "      <td>532981</td>\n",
       "      <td>C1</td>\n",
       "      <td>C17</td>\n",
       "      <td>C1</td>\n",
       "      <td>C20</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>9.106644e+05</td>\n",
       "      <td>5.443690e+04</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215582</th>\n",
       "      <td>532941</td>\n",
       "      <td>E15</td>\n",
       "      <td>M55</td>\n",
       "      <td>V86</td>\n",
       "      <td>X23</td>\n",
       "      <td>J72</td>\n",
       "      <td>R19</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958506e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229634</th>\n",
       "      <td>532961</td>\n",
       "      <td>G38</td>\n",
       "      <td>L49</td>\n",
       "      <td>J67</td>\n",
       "      <td>O92</td>\n",
       "      <td>L36</td>\n",
       "      <td>K45</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230216</th>\n",
       "      <td>532944</td>\n",
       "      <td>L26</td>\n",
       "      <td>Q54</td>\n",
       "      <td>W66</td>\n",
       "      <td>D88</td>\n",
       "      <td>I57</td>\n",
       "      <td>J90</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230755</th>\n",
       "      <td>532951</td>\n",
       "      <td>Z06</td>\n",
       "      <td>H26</td>\n",
       "      <td>W22</td>\n",
       "      <td>Q11</td>\n",
       "      <td>T83</td>\n",
       "      <td>D25</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958506e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246305</th>\n",
       "      <td>532953</td>\n",
       "      <td>P31</td>\n",
       "      <td>Z98</td>\n",
       "      <td>I08</td>\n",
       "      <td>M66</td>\n",
       "      <td>B43</td>\n",
       "      <td>F54</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246501</th>\n",
       "      <td>532914</td>\n",
       "      <td>O64</td>\n",
       "      <td>W24</td>\n",
       "      <td>S79</td>\n",
       "      <td>X72</td>\n",
       "      <td>Z74</td>\n",
       "      <td>J09</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251260</th>\n",
       "      <td>532911</td>\n",
       "      <td>Q52</td>\n",
       "      <td>G37</td>\n",
       "      <td>L09</td>\n",
       "      <td>L03</td>\n",
       "      <td>C31</td>\n",
       "      <td>L31</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261308</th>\n",
       "      <td>532979</td>\n",
       "      <td>T68</td>\n",
       "      <td>A89</td>\n",
       "      <td>L29</td>\n",
       "      <td>H65</td>\n",
       "      <td>I26</td>\n",
       "      <td>Q69</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958502e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281049</th>\n",
       "      <td>532946</td>\n",
       "      <td>H50</td>\n",
       "      <td>I98</td>\n",
       "      <td>F19</td>\n",
       "      <td>M46</td>\n",
       "      <td>O26</td>\n",
       "      <td>T37</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303744</th>\n",
       "      <td>532966</td>\n",
       "      <td>H54</td>\n",
       "      <td>Q11</td>\n",
       "      <td>X82</td>\n",
       "      <td>H69</td>\n",
       "      <td>J21</td>\n",
       "      <td>R89</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306535</th>\n",
       "      <td>532977</td>\n",
       "      <td>J73</td>\n",
       "      <td>K76</td>\n",
       "      <td>Q44</td>\n",
       "      <td>K32</td>\n",
       "      <td>H00</td>\n",
       "      <td>A42</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315589</th>\n",
       "      <td>532920</td>\n",
       "      <td>P79</td>\n",
       "      <td>N56</td>\n",
       "      <td>N39</td>\n",
       "      <td>P47</td>\n",
       "      <td>H17</td>\n",
       "      <td>S52</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318426</th>\n",
       "      <td>532978</td>\n",
       "      <td>O37</td>\n",
       "      <td>J69</td>\n",
       "      <td>P13</td>\n",
       "      <td>K26</td>\n",
       "      <td>S16</td>\n",
       "      <td>H03</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336799</th>\n",
       "      <td>532940</td>\n",
       "      <td>N15</td>\n",
       "      <td>G51</td>\n",
       "      <td>G09</td>\n",
       "      <td>S21</td>\n",
       "      <td>E99</td>\n",
       "      <td>W62</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348343</th>\n",
       "      <td>532945</td>\n",
       "      <td>H84</td>\n",
       "      <td>O65</td>\n",
       "      <td>X59</td>\n",
       "      <td>S63</td>\n",
       "      <td>A81</td>\n",
       "      <td>W03</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359776</th>\n",
       "      <td>533008</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C90</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>9.106513e+05</td>\n",
       "      <td>5.445623e+04</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359909</th>\n",
       "      <td>532925</td>\n",
       "      <td>S12</td>\n",
       "      <td>V98</td>\n",
       "      <td>Y75</td>\n",
       "      <td>X00</td>\n",
       "      <td>W61</td>\n",
       "      <td>O94</td>\n",
       "      <td>9.244554e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365262</th>\n",
       "      <td>532950</td>\n",
       "      <td>N09</td>\n",
       "      <td>A43</td>\n",
       "      <td>U50</td>\n",
       "      <td>C72</td>\n",
       "      <td>I53</td>\n",
       "      <td>F05</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370090</th>\n",
       "      <td>532960</td>\n",
       "      <td>E00</td>\n",
       "      <td>W67</td>\n",
       "      <td>W79</td>\n",
       "      <td>T97</td>\n",
       "      <td>N82</td>\n",
       "      <td>J10</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370360</th>\n",
       "      <td>532962</td>\n",
       "      <td>Y59</td>\n",
       "      <td>O97</td>\n",
       "      <td>B06</td>\n",
       "      <td>T34</td>\n",
       "      <td>R82</td>\n",
       "      <td>Y03</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372775</th>\n",
       "      <td>532926</td>\n",
       "      <td>O67</td>\n",
       "      <td>S07</td>\n",
       "      <td>Q28</td>\n",
       "      <td>O48</td>\n",
       "      <td>T24</td>\n",
       "      <td>H70</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378649</th>\n",
       "      <td>532933</td>\n",
       "      <td>T65</td>\n",
       "      <td>W86</td>\n",
       "      <td>V91</td>\n",
       "      <td>J21</td>\n",
       "      <td>O91</td>\n",
       "      <td>F02</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382827</th>\n",
       "      <td>532915</td>\n",
       "      <td>C91</td>\n",
       "      <td>S91</td>\n",
       "      <td>V74</td>\n",
       "      <td>X71</td>\n",
       "      <td>U05</td>\n",
       "      <td>U92</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386827</th>\n",
       "      <td>532947</td>\n",
       "      <td>T28</td>\n",
       "      <td>V24</td>\n",
       "      <td>K80</td>\n",
       "      <td>V23</td>\n",
       "      <td>P72</td>\n",
       "      <td>W12</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391298</th>\n",
       "      <td>532916</td>\n",
       "      <td>J13</td>\n",
       "      <td>M59</td>\n",
       "      <td>N30</td>\n",
       "      <td>E40</td>\n",
       "      <td>W84</td>\n",
       "      <td>I18</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958502e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397217</th>\n",
       "      <td>532913</td>\n",
       "      <td>I86</td>\n",
       "      <td>Y63</td>\n",
       "      <td>P12</td>\n",
       "      <td>V25</td>\n",
       "      <td>N67</td>\n",
       "      <td>E35</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405600</th>\n",
       "      <td>532919</td>\n",
       "      <td>Z37</td>\n",
       "      <td>J34</td>\n",
       "      <td>U11</td>\n",
       "      <td>G29</td>\n",
       "      <td>Y78</td>\n",
       "      <td>M08</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406420</th>\n",
       "      <td>532952</td>\n",
       "      <td>Z54</td>\n",
       "      <td>P69</td>\n",
       "      <td>L59</td>\n",
       "      <td>C55</td>\n",
       "      <td>W57</td>\n",
       "      <td>D02</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423136</th>\n",
       "      <td>532921</td>\n",
       "      <td>X26</td>\n",
       "      <td>I61</td>\n",
       "      <td>J33</td>\n",
       "      <td>K49</td>\n",
       "      <td>L72</td>\n",
       "      <td>I19</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429599</th>\n",
       "      <td>532949</td>\n",
       "      <td>B31</td>\n",
       "      <td>X34</td>\n",
       "      <td>P17</td>\n",
       "      <td>Q98</td>\n",
       "      <td>F93</td>\n",
       "      <td>Y23</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435165</th>\n",
       "      <td>532929</td>\n",
       "      <td>E59</td>\n",
       "      <td>R76</td>\n",
       "      <td>D30</td>\n",
       "      <td>M02</td>\n",
       "      <td>D16</td>\n",
       "      <td>B35</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438816</th>\n",
       "      <td>532931</td>\n",
       "      <td>T87</td>\n",
       "      <td>V13</td>\n",
       "      <td>R30</td>\n",
       "      <td>C03</td>\n",
       "      <td>F26</td>\n",
       "      <td>G64</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445338</th>\n",
       "      <td>532935</td>\n",
       "      <td>V89</td>\n",
       "      <td>Z78</td>\n",
       "      <td>I94</td>\n",
       "      <td>E15</td>\n",
       "      <td>O46</td>\n",
       "      <td>K87</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461429</th>\n",
       "      <td>532974</td>\n",
       "      <td>U94</td>\n",
       "      <td>K13</td>\n",
       "      <td>K84</td>\n",
       "      <td>U04</td>\n",
       "      <td>N56</td>\n",
       "      <td>N94</td>\n",
       "      <td>9.244550e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466915</th>\n",
       "      <td>532975</td>\n",
       "      <td>F66</td>\n",
       "      <td>W71</td>\n",
       "      <td>M10</td>\n",
       "      <td>F92</td>\n",
       "      <td>B31</td>\n",
       "      <td>F03</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468803</th>\n",
       "      <td>532957</td>\n",
       "      <td>E26</td>\n",
       "      <td>B76</td>\n",
       "      <td>C39</td>\n",
       "      <td>U14</td>\n",
       "      <td>R07</td>\n",
       "      <td>T45</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484997</th>\n",
       "      <td>532943</td>\n",
       "      <td>H45</td>\n",
       "      <td>W38</td>\n",
       "      <td>L17</td>\n",
       "      <td>E19</td>\n",
       "      <td>S43</td>\n",
       "      <td>T94</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958505e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487511</th>\n",
       "      <td>532976</td>\n",
       "      <td>S05</td>\n",
       "      <td>E83</td>\n",
       "      <td>L07</td>\n",
       "      <td>Z63</td>\n",
       "      <td>K43</td>\n",
       "      <td>L70</td>\n",
       "      <td>9.244552e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498175</th>\n",
       "      <td>532963</td>\n",
       "      <td>Z06</td>\n",
       "      <td>G77</td>\n",
       "      <td>Z22</td>\n",
       "      <td>R09</td>\n",
       "      <td>V09</td>\n",
       "      <td>X53</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502255</th>\n",
       "      <td>532954</td>\n",
       "      <td>J38</td>\n",
       "      <td>C30</td>\n",
       "      <td>G28</td>\n",
       "      <td>T38</td>\n",
       "      <td>C62</td>\n",
       "      <td>L03</td>\n",
       "      <td>9.244551e+07</td>\n",
       "      <td>5.958503e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528449</th>\n",
       "      <td>532959</td>\n",
       "      <td>C89</td>\n",
       "      <td>S43</td>\n",
       "      <td>E40</td>\n",
       "      <td>Y34</td>\n",
       "      <td>L29</td>\n",
       "      <td>N28</td>\n",
       "      <td>9.244553e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528759</th>\n",
       "      <td>532910</td>\n",
       "      <td>P36</td>\n",
       "      <td>V48</td>\n",
       "      <td>Z17</td>\n",
       "      <td>J68</td>\n",
       "      <td>Q50</td>\n",
       "      <td>J62</td>\n",
       "      <td>9.244554e+07</td>\n",
       "      <td>5.958504e+07</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BELNR WAERS BUKRS KTOSL PRCTR BSCHL HKONT         DMBTR  \\\n",
       "35691   532967   M07   S42   J81   A14   O49   Z01  9.244553e+07   \n",
       "35944   532917   J82   Q22   P32   Z74   Y19   K27  9.244553e+07   \n",
       "41925   532923   T17   R47   D84   F38   I81   D68  9.244552e+07   \n",
       "52048   532958   Q82   S54   J53   T46   Y42   S06  9.244553e+07   \n",
       "71036   532939   B00   O64   T41   Y68   H15   L79  9.244552e+07   \n",
       "88224   532934   D88   B37   D51   F69   N50   M13  9.244552e+07   \n",
       "90831   532955   P03   M90   U27   R22   S15   N95  9.244551e+07   \n",
       "91688   532932   R07   U18   V96   L28   K79   K77  9.244552e+07   \n",
       "93972   532964   L82   G45   G19   W37   A49   U45  9.244550e+07   \n",
       "102552  532937   P37   S83   S23   U16   N35   C42  9.244552e+07   \n",
       "113179  532969   H01   E65   O50   F20   M83   V78  9.244552e+07   \n",
       "119338  532930   G92   W69   K78   J13   W62   S49  9.244554e+07   \n",
       "131910  532922   O43   R07   N80   J39   T90   V92  9.244552e+07   \n",
       "172868  532973   W25   G35   K00   C11   B38   C25  9.244550e+07   \n",
       "179462  532972   S31   G87   A19   H01   U81   Z02  9.244552e+07   \n",
       "198583  532927   K11   Q00   R19   U99   F24   N10  9.244551e+07   \n",
       "199041  532968   K99   T17   C84   L74   T28   N47  9.244551e+07   \n",
       "213105  532981    C1   C17    C1   C20    A1    B1  9.106644e+05   \n",
       "215582  532941   E15   M55   V86   X23   J72   R19  9.244551e+07   \n",
       "229634  532961   G38   L49   J67   O92   L36   K45  9.244552e+07   \n",
       "230216  532944   L26   Q54   W66   D88   I57   J90  9.244552e+07   \n",
       "230755  532951   Z06   H26   W22   Q11   T83   D25  9.244553e+07   \n",
       "246305  532953   P31   Z98   I08   M66   B43   F54  9.244553e+07   \n",
       "246501  532914   O64   W24   S79   X72   Z74   J09  9.244552e+07   \n",
       "251260  532911   Q52   G37   L09   L03   C31   L31  9.244553e+07   \n",
       "261308  532979   T68   A89   L29   H65   I26   Q69  9.244550e+07   \n",
       "281049  532946   H50   I98   F19   M46   O26   T37  9.244552e+07   \n",
       "303744  532966   H54   Q11   X82   H69   J21   R89  9.244552e+07   \n",
       "306535  532977   J73   K76   Q44   K32   H00   A42  9.244551e+07   \n",
       "315589  532920   P79   N56   N39   P47   H17   S52  9.244551e+07   \n",
       "...        ...   ...   ...   ...   ...   ...   ...           ...   \n",
       "318426  532978   O37   J69   P13   K26   S16   H03  9.244552e+07   \n",
       "336799  532940   N15   G51   G09   S21   E99   W62  9.244553e+07   \n",
       "348343  532945   H84   O65   X59   S63   A81   W03  9.244553e+07   \n",
       "359776  533008    C1   C11    C1   C90    A1    B1  9.106513e+05   \n",
       "359909  532925   S12   V98   Y75   X00   W61   O94  9.244554e+07   \n",
       "365262  532950   N09   A43   U50   C72   I53   F05  9.244551e+07   \n",
       "370090  532960   E00   W67   W79   T97   N82   J10  9.244552e+07   \n",
       "370360  532962   Y59   O97   B06   T34   R82   Y03  9.244551e+07   \n",
       "372775  532926   O67   S07   Q28   O48   T24   H70  9.244552e+07   \n",
       "378649  532933   T65   W86   V91   J21   O91   F02  9.244552e+07   \n",
       "382827  532915   C91   S91   V74   X71   U05   U92  9.244550e+07   \n",
       "386827  532947   T28   V24   K80   V23   P72   W12  9.244553e+07   \n",
       "391298  532916   J13   M59   N30   E40   W84   I18  9.244551e+07   \n",
       "397217  532913   I86   Y63   P12   V25   N67   E35  9.244553e+07   \n",
       "405600  532919   Z37   J34   U11   G29   Y78   M08  9.244551e+07   \n",
       "406420  532952   Z54   P69   L59   C55   W57   D02  9.244552e+07   \n",
       "423136  532921   X26   I61   J33   K49   L72   I19  9.244552e+07   \n",
       "429599  532949   B31   X34   P17   Q98   F93   Y23  9.244552e+07   \n",
       "435165  532929   E59   R76   D30   M02   D16   B35  9.244551e+07   \n",
       "438816  532931   T87   V13   R30   C03   F26   G64  9.244553e+07   \n",
       "445338  532935   V89   Z78   I94   E15   O46   K87  9.244550e+07   \n",
       "461429  532974   U94   K13   K84   U04   N56   N94  9.244550e+07   \n",
       "466915  532975   F66   W71   M10   F92   B31   F03  9.244553e+07   \n",
       "468803  532957   E26   B76   C39   U14   R07   T45  9.244552e+07   \n",
       "484997  532943   H45   W38   L17   E19   S43   T94  9.244551e+07   \n",
       "487511  532976   S05   E83   L07   Z63   K43   L70  9.244552e+07   \n",
       "498175  532963   Z06   G77   Z22   R09   V09   X53  9.244553e+07   \n",
       "502255  532954   J38   C30   G28   T38   C62   L03  9.244551e+07   \n",
       "528449  532959   C89   S43   E40   Y34   L29   N28  9.244553e+07   \n",
       "528759  532910   P36   V48   Z17   J68   Q50   J62  9.244554e+07   \n",
       "\n",
       "               WRBTR   label  \n",
       "35691   5.958503e+07  global  \n",
       "35944   5.958504e+07  global  \n",
       "41925   5.958505e+07  global  \n",
       "52048   5.958503e+07  global  \n",
       "71036   5.958503e+07  global  \n",
       "88224   5.958504e+07  global  \n",
       "90831   5.958504e+07  global  \n",
       "91688   5.958504e+07  global  \n",
       "93972   5.958504e+07  global  \n",
       "102552  5.958502e+07  global  \n",
       "113179  5.958503e+07  global  \n",
       "119338  5.958505e+07  global  \n",
       "131910  5.958504e+07  global  \n",
       "172868  5.958505e+07  global  \n",
       "179462  5.958503e+07  global  \n",
       "198583  5.958506e+07  global  \n",
       "199041  5.958501e+07  global  \n",
       "213105  5.443690e+04   local  \n",
       "215582  5.958506e+07  global  \n",
       "229634  5.958503e+07  global  \n",
       "230216  5.958505e+07  global  \n",
       "230755  5.958506e+07  global  \n",
       "246305  5.958503e+07  global  \n",
       "246501  5.958504e+07  global  \n",
       "251260  5.958505e+07  global  \n",
       "261308  5.958502e+07  global  \n",
       "281049  5.958505e+07  global  \n",
       "303744  5.958505e+07  global  \n",
       "306535  5.958505e+07  global  \n",
       "315589  5.958503e+07  global  \n",
       "...              ...     ...  \n",
       "318426  5.958504e+07  global  \n",
       "336799  5.958505e+07  global  \n",
       "348343  5.958505e+07  global  \n",
       "359776  5.445623e+04   local  \n",
       "359909  5.958504e+07  global  \n",
       "365262  5.958504e+07  global  \n",
       "370090  5.958504e+07  global  \n",
       "370360  5.958504e+07  global  \n",
       "372775  5.958503e+07  global  \n",
       "378649  5.958504e+07  global  \n",
       "382827  5.958503e+07  global  \n",
       "386827  5.958504e+07  global  \n",
       "391298  5.958502e+07  global  \n",
       "397217  5.958505e+07  global  \n",
       "405600  5.958505e+07  global  \n",
       "406420  5.958505e+07  global  \n",
       "423136  5.958505e+07  global  \n",
       "429599  5.958504e+07  global  \n",
       "435165  5.958504e+07  global  \n",
       "438816  5.958504e+07  global  \n",
       "445338  5.958504e+07  global  \n",
       "461429  5.958504e+07  global  \n",
       "466915  5.958504e+07  global  \n",
       "468803  5.958504e+07  global  \n",
       "484997  5.958505e+07  global  \n",
       "487511  5.958503e+07  global  \n",
       "498175  5.958504e+07  global  \n",
       "502255  5.958503e+07  global  \n",
       "528449  5.958504e+07  global  \n",
       "528759  5.958504e+07  global  \n",
       "\n",
       "[61 rows x 10 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset[reconstruction_loss_transaction >= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "global    59\n",
       "local      2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset[reconstruction_loss_transaction >= 0.1].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BELNR</th>\n",
       "      <th>WAERS</th>\n",
       "      <th>BUKRS</th>\n",
       "      <th>KTOSL</th>\n",
       "      <th>PRCTR</th>\n",
       "      <th>BSCHL</th>\n",
       "      <th>HKONT</th>\n",
       "      <th>DMBTR</th>\n",
       "      <th>WRBTR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12939</th>\n",
       "      <td>532980</td>\n",
       "      <td>C1</td>\n",
       "      <td>C20</td>\n",
       "      <td>C1</td>\n",
       "      <td>C18</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910658.284578</td>\n",
       "      <td>54449.838820</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32317</th>\n",
       "      <td>532989</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C53</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910653.005238</td>\n",
       "      <td>54439.211421</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34059</th>\n",
       "      <td>533003</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C64</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910631.632279</td>\n",
       "      <td>54443.283794</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43399</th>\n",
       "      <td>532998</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C79</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910650.440131</td>\n",
       "      <td>54435.055247</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97954</th>\n",
       "      <td>532984</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C68</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910648.636111</td>\n",
       "      <td>54442.618111</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144286</th>\n",
       "      <td>532994</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C76</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910645.076191</td>\n",
       "      <td>54448.440199</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170320</th>\n",
       "      <td>533001</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C30</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910645.965114</td>\n",
       "      <td>54452.809780</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220927</th>\n",
       "      <td>533002</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C60</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910648.545092</td>\n",
       "      <td>54446.382830</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234335</th>\n",
       "      <td>532987</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C80</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910647.596802</td>\n",
       "      <td>54446.113055</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234460</th>\n",
       "      <td>532988</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C26</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910655.728514</td>\n",
       "      <td>54441.600292</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300115</th>\n",
       "      <td>533000</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C88</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910649.135220</td>\n",
       "      <td>54441.440806</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306995</th>\n",
       "      <td>532990</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C39</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910648.451982</td>\n",
       "      <td>54428.498315</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318771</th>\n",
       "      <td>533004</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C66</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910657.152500</td>\n",
       "      <td>54466.259351</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326531</th>\n",
       "      <td>533005</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C24</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910632.886549</td>\n",
       "      <td>54458.195111</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352339</th>\n",
       "      <td>532997</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C71</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910666.808640</td>\n",
       "      <td>54445.734669</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360246</th>\n",
       "      <td>532983</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C72</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910639.376486</td>\n",
       "      <td>54456.999522</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364695</th>\n",
       "      <td>533009</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C32</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910658.458544</td>\n",
       "      <td>54432.742656</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421175</th>\n",
       "      <td>533007</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C78</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910635.287793</td>\n",
       "      <td>54442.688594</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462643</th>\n",
       "      <td>532982</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C21</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910648.876816</td>\n",
       "      <td>54437.855551</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463973</th>\n",
       "      <td>532992</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C57</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910649.925467</td>\n",
       "      <td>54442.879962</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502056</th>\n",
       "      <td>532995</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C81</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910659.449029</td>\n",
       "      <td>54432.920178</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507636</th>\n",
       "      <td>532993</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C70</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910676.587040</td>\n",
       "      <td>54441.755167</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532375</th>\n",
       "      <td>532985</td>\n",
       "      <td>C1</td>\n",
       "      <td>C11</td>\n",
       "      <td>C1</td>\n",
       "      <td>C75</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>910650.508962</td>\n",
       "      <td>54449.483129</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BELNR WAERS BUKRS KTOSL PRCTR BSCHL HKONT          DMBTR  \\\n",
       "12939   532980    C1   C20    C1   C18    A1    B1  910658.284578   \n",
       "32317   532989    C1   C11    C1   C53    A1    B1  910653.005238   \n",
       "34059   533003    C1   C11    C1   C64    A1    B1  910631.632279   \n",
       "43399   532998    C1   C11    C1   C79    A1    B1  910650.440131   \n",
       "97954   532984    C1   C11    C1   C68    A1    B1  910648.636111   \n",
       "144286  532994    C1   C11    C1   C76    A1    B1  910645.076191   \n",
       "170320  533001    C1   C11    C1   C30    A1    B1  910645.965114   \n",
       "220927  533002    C1   C11    C1   C60    A1    B1  910648.545092   \n",
       "234335  532987    C1   C11    C1   C80    A1    B1  910647.596802   \n",
       "234460  532988    C1   C11    C1   C26    A1    B1  910655.728514   \n",
       "300115  533000    C1   C11    C1   C88    A1    B1  910649.135220   \n",
       "306995  532990    C1   C11    C1   C39    A1    B1  910648.451982   \n",
       "318771  533004    C1   C11    C1   C66    A1    B1  910657.152500   \n",
       "326531  533005    C1   C11    C1   C24    A1    B1  910632.886549   \n",
       "352339  532997    C1   C11    C1   C71    A1    B1  910666.808640   \n",
       "360246  532983    C1   C11    C1   C72    A1    B1  910639.376486   \n",
       "364695  533009    C1   C11    C1   C32    A1    B1  910658.458544   \n",
       "421175  533007    C1   C11    C1   C78    A1    B1  910635.287793   \n",
       "462643  532982    C1   C11    C1   C21    A1    B1  910648.876816   \n",
       "463973  532992    C1   C11    C1   C57    A1    B1  910649.925467   \n",
       "502056  532995    C1   C11    C1   C81    A1    B1  910659.449029   \n",
       "507636  532993    C1   C11    C1   C70    A1    B1  910676.587040   \n",
       "532375  532985    C1   C11    C1   C75    A1    B1  910650.508962   \n",
       "\n",
       "               WRBTR  label  \n",
       "12939   54449.838820  local  \n",
       "32317   54439.211421  local  \n",
       "34059   54443.283794  local  \n",
       "43399   54435.055247  local  \n",
       "97954   54442.618111  local  \n",
       "144286  54448.440199  local  \n",
       "170320  54452.809780  local  \n",
       "220927  54446.382830  local  \n",
       "234335  54446.113055  local  \n",
       "234460  54441.600292  local  \n",
       "300115  54441.440806  local  \n",
       "306995  54428.498315  local  \n",
       "318771  54466.259351  local  \n",
       "326531  54458.195111  local  \n",
       "352339  54445.734669  local  \n",
       "360246  54456.999522  local  \n",
       "364695  54432.742656  local  \n",
       "421175  54442.688594  local  \n",
       "462643  54437.855551  local  \n",
       "463973  54442.879962  local  \n",
       "502056  54432.920178  local  \n",
       "507636  54441.755167  local  \n",
       "532375  54449.483129  local  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset[(reconstruction_loss_transaction >= 0.018) & (reconstruction_loss_transaction < 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local    23\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_dataset[(reconstruction_loss_transaction >= 0.018) & (reconstruction_loss_transaction < 0.05)].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
